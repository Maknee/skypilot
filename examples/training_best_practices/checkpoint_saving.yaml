# DeepSeek Distilled Models Checkpoint Saving Benchmark
#
# Usage:
#
#  S3_BUCKET=your-bucket sky launch checkpoint_saving.yaml -c deepseek-save-benchmark
#
# This config benchmarks checkpoint saving performance for DeepSeek distilled models across:
# 1. NVMe storage (local fast storage)
# 2. S3 mounted storage (FUSE-mounted S3 with MOUNT mode)  
# 3. S3 mounted storage (FUSE-mounted S3 with MOUNT_CACHED mode)
# 4. Nebius shared filesystem (distributed storage)
# 5. S3 direct upload (not mounted, using boto3/s3fs)
#
# Download strategy: Models are downloaded to fast NVMe storage first, then available
# for checkpoint saving benchmarks across different storage backends.
#
# Note: Requires ~500GB+ disk space for downloading all DeepSeek models
#
# Example Configurations:
#
# Small models only (for testing):
#   TEST_MODELS="bert-base-uncased,distilbert-base-uncased"
#   LOCAL_MODEL_DIRS="bert-base-uncased,distilbert-base-uncased"
#
# Only large models:
#   TEST_MODELS="deepseek-ai/DeepSeek-R1-Distill-Qwen-32B,deepseek-ai/DeepSeek-R1-Distill-Llama-70B"
#   LOCAL_MODEL_DIRS="deepseek-32b,deepseek-70b"
#
# Skip downloads (benchmark only):
#   DOWNLOAD_MODELS="false"
#
# Test specific checkpoint formats:
#   CHECKPOINT_FORMATS="torch"  # or "safetensors" or "torch,safetensors"

envs:
  S3_BUCKET: nebius://henry-test-et  # S3 bucket for checkpoints
  S3_BUCKET2: nebius://henry-test  # S3 bucket for checkpoints
  HF_TOKEN:  # Add your HuggingFace token if needed
  
  # Configure which models to test (comma-separated DeepSeek distilled models)
  # TEST_MODELS: "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B,deepseek-ai/DeepSeek-R1-Distill-Qwen-14B,deepseek-ai/DeepSeek-R1-Distill-Qwen-32B,deepseek-ai/DeepSeek-R1-Distill-Llama-70B"
  TEST_MODELS: "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
  # TEST_MODELS: "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
  
  # Local directory names for each model (comma-separated, same order as TEST_MODELS)
  # LOCAL_MODEL_DIRS: "deepseek-7b,deepseek-14b,deepseek-32b,deepseek-70b"
  LOCAL_MODEL_DIRS: "deepseek-7b"
  # LOCAL_MODEL_DIRS: "deepseek-32b"
  
  # Benchmark configurations
  SAVE_ITERATIONS: "1"  # Number of save iterations per benchmark
  
  # Control which models to download (set to "false" to skip expensive downloads)
  DOWNLOAD_MODELS: "true"

resources:
  # cloud: aws
  # region: us-west-2  # Ensure compute is in same region as S3 bucket
  cpus: 32+
  memory: 256+
  # accelerators: H100:8
  network_tier: best
  disk_tier: best
  disk_size: 2000

num_nodes: 1

# Configure S3 mount modes for comparison
file_mounts:
  /checkpoints_s3_mount:
    source: ${S3_BUCKET}
    mode: MOUNT  # Standard MOUNT mode
  /checkpoints_s3_mount_cached:
    source: ${S3_BUCKET2}
    mode: MOUNT_CACHED  # MOUNT_CACHED mode will intelligently cache for faster writes
  # Mount the saving benchmark script
  /app/save_benchmark.py: ./checkpoint/save_benchmark.py

volumes:
  # Mount the Nebius shared filesystem to /mnt/data across all nodes
  /mnt/data: nebius-pvc

setup: |
  # Test VM cache dropping capability for fair benchmarking
  echo "Testing VM cache dropping capability..."
  if sudo sh -c 'echo 3 > /proc/sys/vm/drop_caches'; then
    echo "‚úÖ VM cache dropping capability confirmed"
  else
    echo "‚ö†Ô∏è VM cache dropping not available - common in containerized environments"
    echo "üí° This is due to read-only /proc/sys filesystem for security"
    echo "üí° Benchmarks will continue but may be affected by OS caching"
  fi

  # Install required packages
  pip install torch torchvision transformers accelerate datasets huggingface_hub
  pip install safetensors boto3 s3fs  # For different saving formats and S3 access
  pip install nvitop psutil
  pip install protobuf  # Required for DeepSeek models
  pip install sentencepiece  # Required for some tokenizers
  sudo apt install -y htop vim sysstat vmtouch

  # Create benchmark directories
  mkdir -p /tmp/checkpoints_nvme     # NVMe storage
  mkdir -p /mnt/data/checkpoints_save     # Nebius shared filesystem
  mkdir -p /app
  
  # Make benchmark scripts executable
  chmod +x /app/save_benchmark.py

  echo "=== Setting up DeepSeek Checkpoint Saving Benchmark ==="
  echo "S3 Bucket: $S3_BUCKET"
  echo "Test Models: $TEST_MODELS"
  echo "Save Iterations: $SAVE_ITERATIONS"
  echo "Using standard save_pretrained() method"
  
  # Set HF token if provided
  if [ ! -z "$HF_TOKEN" ]; then
    huggingface-cli login --token $HF_TOKEN
  fi
  
  # Check available resources
  echo "=== System Resources ==="
  nvidia-smi
  free -h
  df -h /tmp
  df -h /checkpoints_s3_mount_cached
  df -h /checkpoints_s3_mount
  df -h /mnt/data
  
  echo "=== Downloading DeepSeek models for saving benchmarks ==="
  
  # Check if we should download models
  if [ "$DOWNLOAD_MODELS" = "false" ]; then
    echo "Model download disabled via DOWNLOAD_MODELS=false"
  else
    # Parse the model lists from environment variables
    IFS=',' read -ra MODELS <<< "$TEST_MODELS"
    IFS=',' read -ra DIRS <<< "$LOCAL_MODEL_DIRS"
    
    # Download each model to NVMe first, then copy to S3 if needed
    for i in "${!MODELS[@]}"; do
      model="${MODELS[$i]}"
      local_dir="${DIRS[$i]}"
      nvme_dir="/tmp/checkpoints_nvme/$local_dir"
      s3_cached_dir="/checkpoints_s3_mount_cached/$local_dir"
      s3_mount_dir="/checkpoints_s3_mount/$local_dir"
      nebius_dir="/mnt/data/checkpoints/$local_dir"
      
      echo "üîΩ Downloading $model to NVMe: $nvme_dir..."
      
      # Always download to NVMe first (or check if already there)
      if [ -d "$nvme_dir" ] && [ -f "$nvme_dir/config.json" ]; then
        # Check if NVMe model has actual weights
        if ls "$nvme_dir"/*.safetensors 1> /dev/null 2>&1 || ls "$nvme_dir"/*.bin 1> /dev/null 2>&1 || ls "$nvme_dir"/model-*.safetensors 1> /dev/null 2>&1; then
          echo "‚úÖ Model $model already exists in NVMe at $nvme_dir"
          du -sh "$nvme_dir"
        else
          echo "‚ö†Ô∏è NVMe model incomplete, will re-download..."
          rm -rf "$nvme_dir"
          mkdir -p "$nvme_dir"
          
          # Download to NVMe
          if huggingface-cli download "$model" --local-dir "$nvme_dir" --local-dir-use-symlinks False --resume-download; then
            echo "‚úÖ Successfully downloaded $model to NVMe"
            du -sh "$nvme_dir"
          else
            echo "‚ùå Failed to download $model to NVMe"
            continue
          fi
        fi
      else
        # Download to NVMe
        rm -rf "$nvme_dir"
        mkdir -p "$nvme_dir"
        
        if huggingface-cli download "$model" --local-dir "$nvme_dir" --local-dir-use-symlinks False --resume-download; then
          echo "‚úÖ Successfully downloaded $model to NVMe"
          du -sh "$nvme_dir"
        else
          echo "‚ùå Failed to download $model to NVMe"
          continue
        fi
      fi
      
      # Now check if S3 MOUNT_CACHED needs a copy
      if [ -d "$s3_cached_dir" ] && [ -f "$s3_cached_dir/config.json" ]; then
        if ls "$s3_cached_dir"/*.safetensors 1> /dev/null 2>&1 || ls "$s3_cached_dir"/*.bin 1> /dev/null 2>&1 || ls "$s3_cached_dir"/model-*.safetensors 1> /dev/null 2>&1; then
          echo "‚úÖ Model already exists in S3 MOUNT_CACHED, skipping copy"
          du -sh "$s3_cached_dir"
        else
          echo "üìã Copying from NVMe to S3 MOUNT_CACHED..."
          rm -rf "$s3_cached_dir"
          mkdir -p "$s3_cached_dir"
          if cp -r "$nvme_dir"/* "$s3_cached_dir"/; then
            echo "‚úÖ Successfully copied to S3 MOUNT_CACHED"
            du -sh "$s3_cached_dir"
          else
            echo "‚ö†Ô∏è Failed to copy to S3 MOUNT_CACHED"
          fi
        fi
      else
        echo "üìã Copying from NVMe to S3 MOUNT_CACHED..."
        rm -rf "$s3_cached_dir"
        mkdir -p "$s3_cached_dir"
        if cp -r "$nvme_dir"/* "$s3_cached_dir"/; then
          echo "‚úÖ Successfully copied to S3 MOUNT_CACHED"
          du -sh "$s3_cached_dir"
        else
          echo "‚ö†Ô∏è Failed to copy to S3 MOUNT_CACHED"
        fi
      fi
      
      # Now check if S3 MOUNT needs a copy  
      if [ -d "$s3_mount_dir" ] && [ -f "$s3_mount_dir/config.json" ]; then
        if ls "$s3_mount_dir"/*.safetensors 1> /dev/null 2>&1 || ls "$s3_mount_dir"/*.bin 1> /dev/null 2>&1 || ls "$s3_mount_dir"/model-*.safetensors 1> /dev/null 2>&1; then
          echo "‚úÖ Model already exists in S3 MOUNT, skipping copy"
          du -sh "$s3_mount_dir"
        else
          echo "üìã Copying from NVMe to S3 MOUNT..."
          rm -rf "$s3_mount_dir"
          mkdir -p "$s3_mount_dir"
          if cp -r "$nvme_dir"/* "$s3_mount_dir"/; then
            echo "‚úÖ Successfully copied to S3 MOUNT"
            du -sh "$s3_mount_dir"
          else
            echo "‚ö†Ô∏è Failed to copy to S3 MOUNT"
          fi
        fi
      else
        echo "üìã Copying from NVMe to S3 MOUNT..."
        rm -rf "$s3_mount_dir"
        mkdir -p "$s3_mount_dir"
        if cp -r "$nvme_dir"/* "$s3_mount_dir"/; then
          echo "‚úÖ Successfully copied to S3 MOUNT"
          du -sh "$s3_mount_dir"
        else
          echo "‚ö†Ô∏è Failed to copy to S3 MOUNT"
        fi
      fi

      # Now check if Nebius needs a copy  
      if [ -d "$nebius_dir" ] && [ -f "$nebius_dir/config.json" ]; then
        if ls "$nebius_dir"/*.safetensors 1> /dev/null 2>&1 || ls "$nebius_dir"/*.bin 1> /dev/null 2>&1 || ls "$nebius_dir"/model-*.safetensors 1> /dev/null 2>&1; then
          echo "‚úÖ Model already exists in Nebius, skipping copy"
          du -sh "$nebius_dir"
        else
          echo "üìã Copying from NVMe to Nebius..."
          rm -rf "$nebius_dir"
          mkdir -p "$nebius_dir"
          if cp -r "$nvme_dir"/* "$nebius_dir"/; then
            echo "‚úÖ Successfully copied to Nebius"
            du -sh "$nebius_dir"
          else
            echo "‚ö†Ô∏è Failed to copy to Nebius"
          fi
        fi
      else
        echo "üìã Copying from NVMe to Nebius..."
        rm -rf "$nebius_dir"
        mkdir -p "$nebius_dir"
        if cp -r "$nvme_dir"/* "$nebius_dir"/; then
          echo "‚úÖ Successfully copied to Nebius"
          du -sh "$nebius_dir"
        else
          echo "‚ö†Ô∏è Failed to copy to Nebius"
        fi
      fi
    done
  fi
  
  echo "‚úÖ DeepSeek checkpoint saving benchmark setup complete"
  
  echo "=== Disk usage summary ==="
  du -sh /checkpoints_s3_mount_cached/* 2>/dev/null || echo "No S3 MOUNT_CACHED models"
  du -sh /checkpoints_s3_mount/* 2>/dev/null || echo "No S3 MOUNT models"
  du -sh /tmp/checkpoints_nvme/* 2>/dev/null || echo "No NVMe models"
  du -sh /mnt/data/checkpoints/* 2>/dev/null || echo "No Nebius models"

run: |
  cd /app
  
  echo "=== Starting DeepSeek Checkpoint Saving Benchmark ==="
  echo "Testing 4 storage backends for checkpoint saving:"
  echo "  1. NVMe Storage: /tmp/checkpoints_nvme"
  echo "  2. S3 MOUNT: /checkpoints_s3_mount (FUSE MOUNT mode)"
  echo "  3. S3 MOUNT_CACHED: /checkpoints_s3_mount_cached (FUSE MOUNT_CACHED mode)"
  echo "  4. Nebius Shared FS: /mnt/data/checkpoints (Distributed storage)"
  echo "Models to test: $TEST_MODELS"
  echo "Always loading models from NVMe for consistent baseline"
  
  # Run comprehensive checkpoint saving benchmark with all storage backends
  python3 save_benchmark.py \
    --test_models "${TEST_MODELS}" \
    --local_model_dirs "${LOCAL_MODEL_DIRS}" \
    --nvme_checkpoint_dir /tmp/checkpoints_nvme \
    --s3_mount_dir /checkpoints_s3_mount \
    --s3_mount_cached_dir /checkpoints_s3_mount_cached \
    --nebius_dir /mnt/data/checkpoints \
    --save_iterations ${SAVE_ITERATIONS} \
    --output_results /tmp/save_benchmark_results.json
  
  echo "=== DeepSeek Checkpoint Saving Benchmark Complete ==="
  
  # Display results
  if [ -f "/tmp/save_benchmark_results.json" ]; then
    echo "=== FINAL CHECKPOINT SAVING RESULTS ==="
    cat /tmp/save_benchmark_results.json | python3 -m json.tool
  fi
  
  # Generate performance comparison report
  echo "=== Performance Comparison Report ==="
  python3 save_benchmark.py \
    --analyze_results_only \
    --results_file /tmp/save_benchmark_results.json \
    --generate_detailed_report
  
  # Show storage usage and performance summary
  echo "=== Storage Usage Summary ==="
  echo "NVMe Storage:"
  du -sh /tmp/checkpoints_nvme/* 2>/dev/null || echo "No NVMe checkpoints"
  echo "S3 MOUNT Storage:"
  du -sh /checkpoints_s3_mount/* 2>/dev/null || echo "No S3 MOUNT checkpoints"
  echo "S3 MOUNT_CACHED Storage:"
  du -sh /checkpoints_s3_mount_cached/* 2>/dev/null || echo "No S3 MOUNT_CACHED checkpoints"
  echo "Nebius Shared FS Storage:"
  du -sh /mnt/data/checkpoints/* 2>/dev/null || echo "No Nebius checkpoints"
  
  echo "=== Final System Resources ==="
  nvidia-smi
  free -h
  
  echo "=== All Checkpoint Saving Benchmarks Complete ==="
  echo "Results saved in:"
  echo "  - Checkpoint saving results: /tmp/save_benchmark_results.json"

config:
  kubernetes:
    pod_config:
      spec:
        containers:
        - securityContext:
            capabilities:
              add:
              - IPC_LOCK
              - SYS_ADMIN
