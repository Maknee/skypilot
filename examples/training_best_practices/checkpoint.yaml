# Comprehensive model loading benchmarking with multiple storage methods
#
# Usage:
#
#  S3_BUCKET=your-bucket sky launch checkpoint.yaml -c checkpoint-benchmark
#
# This config compares 3 different storage methods with 6 total configurations:
# 1. S3 mount loading (standard + parallel workers)
# 2. NVMe loading (standard + parallel workers)  
# 3. Nebius shared filesystem loading (standard + parallel workers)
#
# Download strategy: Models are downloaded to fast NVMe storage first, then copied
# to S3 bucket and Nebius shared filesystem. This avoids S3 FUSE mount issues with large model downloads.
#
# VM caches are dropped before each test when possible (may not work in some
# containerized environments due to read-only /proc/sys filesystem).
# Tests DeepSeek models (7B, 14B, 32B, 70B) for multi-model loading analysis
#
# Note: Requires ~500GB+ disk space for downloading all DeepSeek models
#
# Example Configurations:
#
# Small models only (for testing):
#   TEST_MODELS="bert-base-uncased,distilbert-base-uncased"
#   LOCAL_MODEL_DIRS="bert-base-uncased,distilbert-base-uncased"
#
# Only large models:
#   TEST_MODELS="deepseek-ai/DeepSeek-R1-Distill-Qwen-32B,deepseek-ai/DeepSeek-R1-Distill-Llama-70B"
#   LOCAL_MODEL_DIRS="deepseek-32b,deepseek-70b"
#
# Skip downloads (benchmark only):
#   DOWNLOAD_MODELS="false"
#
# Disable cache dropping (if failing):
#   DISABLE_CACHE_DROP="true"
#
# Use legacy four-config benchmark (S3 + NVMe only):
#   FOUR_CONFIG_ONLY="true"

envs:
  S3_BUCKET: nebius://henry-test-et  # S3 bucket for checkpoints
  HF_TOKEN:  # Add your HuggingFace token if needed
  HF_ENABLE_PARALLEL_LOADING: "true"  # Enable HuggingFace parallel loading
  HF_PARALLEL_LOADING_WORKERS: "8"    # Number of parallel workers
  
  # AWS credentials for S3:// URI loading will be pulled from:
  # 1. Environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION)
  # 2. IAM instance roles (if running on AWS)
  # 3. ~/.aws/credentials file
  # 4. AWS CLI configuration
  
  # Configure which models to test (comma-separated)
  # TEST_MODELS: "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B,deepseek-ai/DeepSeek-R1-Distill-Qwen-14B,deepseek-ai/DeepSeek-R1-Distill-Qwen-32B,deepseek-ai/DeepSeek-R1-Distill-Llama-70B"
  # TEST_MODELS: "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B,deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
  TEST_MODELS: "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
  # TEST_MODELS: "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
  
  # Local directory names for each model (comma-separated, same order as TEST_MODELS)
  # LOCAL_MODEL_DIRS: "deepseek-7b,deepseek-14b,deepseek-32b,deepseek-70b"
  LOCAL_MODEL_DIRS: "deepseek-7b"
  # LOCAL_MODEL_DIRS: "deepseek-7b,deepseek-14b"
  # LOCAL_MODEL_DIRS: "deepseek-32b"
  
  # Control which models to download (set to "false" to skip expensive downloads)
  DOWNLOAD_MODELS: "true"
  
  # Disable VM cache dropping if it fails in containerized environments
  DISABLE_CACHE_DROP: "false"
  
  # Use legacy four-config benchmark (S3 + NVMe only, no Nebius)
  # FOUR_CONFIG_ONLY: "true"

resources:
  # cloud: aws
  # region: us-west-2  # Ensure compute is in same region as S3 bucket
  cpus: 32+
  memory: 256+
  # accelerators: H100:8
  network_tier: best
  disk_tier: best
  disk_size: 2000

num_nodes: 1

# Configure buckets for dataset and checkpoints with both S3 and local NVMe
file_mounts:
  /checkpoints_s3:
    source: ${S3_BUCKET}
    mode: MOUNT
    # mode: MOUNT_CACHED  # MOUNT_CACHED mode will intelligently cache the checkpoint for faster writes
  # Mount the training script
  /app/train.py: ./checkpoint/train.py

volumes:
  # Mount the Nebius shared filesystem to /mnt/data across all nodes
  /mnt/data: nebius-pvc

setup: |
  # Test VM cache dropping capability for fair benchmarking
  echo "Testing VM cache dropping capability..."
  if sudo sh -c 'echo 3 > /proc/sys/vm/drop_caches'; then
    echo "‚úÖ VM cache dropping capability confirmed"
  else
    echo "‚ö†Ô∏è VM cache dropping not available - common in containerized environments"
    echo "üí° This is due to read-only /proc/sys filesystem for security"
    echo "üí° Benchmarks will continue but may be affected by OS caching"
    echo "üí° Set DISABLE_CACHE_DROP=true to skip cache dropping attempts"
  fi

  # Install required packages
  pip install torch torchvision transformers accelerate datasets huggingface_hub
  pip install nvitop
  pip install protobuf  # Required for DeepSeek models
  pip install sentencepiece  # Required for some tokenizers
  pip install safetensors  # For loading safetensors format models
  pip install boto3  # Required for S3:// URI loading
  sudo apt install -y htop vim sysstat vmtouch

  # Create local NVMe checkpoint directory
  mkdir -p /tmp/checkpoints_nvme
  mkdir -p /app
  
  # Make training script executable
  chmod +x /app/train.py
  
  echo "=== Downloading DeepSeek models for multi-model benchmarking ==="
  
  # Check if we should download models
  if [ "$DOWNLOAD_MODELS" = "false" ]; then
    echo "Model download disabled via DOWNLOAD_MODELS=false"
  else
    # Set HF token if provided (needed for some models)
    if [ ! -z "$HF_TOKEN" ]; then
      huggingface-cli login --token $HF_TOKEN
    fi
    
    # Parse the model lists from environment variables
    IFS=',' read -ra MODELS <<< "$TEST_MODELS"
    IFS=',' read -ra DIRS <<< "$LOCAL_MODEL_DIRS"
    
    # Download each model
    for i in "${!MODELS[@]}"; do
      model="${MODELS[$i]}"
      local_dir="${DIRS[$i]}"
      nvme_dir="/tmp/checkpoints_nvme/$local_dir"
      s3_dir="/checkpoints_s3/$local_dir"
      
      # Check if model already exists in S3 and is complete
      if [ -d "$s3_dir" ] && [ -f "$s3_dir/config.json" ]; then
        # Check if S3 model has actual weights (safetensors or bin files)
        if ls "$s3_dir"/*.safetensors 1> /dev/null 2>&1 || ls "$s3_dir"/*.bin 1> /dev/null 2>&1 || ls "$s3_dir"/model-*.safetensors 1> /dev/null 2>&1; then
          echo "‚úÖ Model $model already exists in S3 at $s3_dir, skipping download"
          du -sh "$s3_dir"
          continue
        else
          echo "‚ö†Ô∏è S3 model incomplete, will re-download..."
        fi
      fi
      
      echo "üîΩ Downloading $model to NVMe first: $nvme_dir..."
      
      # Clean up any existing partial downloads
      rm -rf "$nvme_dir"
      mkdir -p "$nvme_dir"
      
      # Download to NVMe first (much more reliable)
      if huggingface-cli download "$model" --local-dir "$nvme_dir" --local-dir-use-symlinks False --resume-download; then
        # Verify download was successful
        if [ -f "$nvme_dir/config.json" ] && (ls "$nvme_dir"/*.safetensors 1> /dev/null 2>&1 || ls "$nvme_dir"/*.bin 1> /dev/null 2>&1 || ls "$nvme_dir"/model-*.safetensors 1> /dev/null 2>&1); then
          echo "‚úÖ Successfully downloaded $model to NVMe"
          du -sh "$nvme_dir"
          echo "üìÅ Model files:"
          find "$nvme_dir" -name "*.safetensors" -o -name "*.bin" -o -name "config.json" | head -10
          
          echo "üìã Copying from NVMe to S3 bucket..."
          copy_start=$(date +%s)
          
          # Clean up S3 directory if it exists
          rm -rf "$s3_dir"
          mkdir -p "$s3_dir"
          
          # Copy from NVMe to S3
          if cp -r "$nvme_dir"/* "$s3_dir"/; then
            copy_end=$(date +%s)
            copy_time=$((copy_end - copy_start))
            echo "‚úÖ Successfully copied to S3 in ${copy_time}s"
            du -sh "$s3_dir"
          else
            echo "‚ö†Ô∏è Failed to copy to S3, but NVMe version available"
          fi
          
        else
          echo "‚ùå Download completed but model weights missing"
          echo "Available files in $nvme_dir:"
          ls -la "$nvme_dir"
        fi
      else
        echo "‚ùå Failed to download $model to NVMe"
      fi
    done
  fi
  
  echo "‚úÖ Setup complete. Available models in S3:"
  find /checkpoints_s3 -name "*.json" -o -name "*.bin" -o -name "*.safetensors" | head -20
  
  echo "=== Disk usage summary ==="
  du -sh /checkpoints_s3/*

run: |
  cd /app
  
  echo "=== Starting Six-Configuration Model Loading Benchmark ==="
  echo "Testing 6 different configurations:"
  echo "  1. S3 Mount (Standard Loading)"
  echo "  2. S3 Mount (Parallel Loading)"
  echo "  3. NVMe (Standard Loading)"
  echo "  4. NVMe (Parallel Loading)"
  echo "  5. Nebius Shared Filesystem (Standard Loading)"
  echo "  6. Nebius Shared Filesystem (Parallel Loading)"
  echo "Models to test: $TEST_MODELS"
  echo "Download strategy: NVMe first ‚Üí S3 copy ‚Üí Nebius copy"
  
  # List available models and checkpoints
  echo "Available models in S3:"
  ls -la /checkpoints_s3/
  
  echo "Nebius filesystem status:"
  ls -la /mnt/data/
  
  # Run six-configuration benchmark by default (or four-config if FOUR_CONFIG_ONLY is set)
  if [ "$FOUR_CONFIG_ONLY" = "true" ]; then
    echo "=== Four-Configuration Loading Benchmark (Legacy) ==="
    python3 train.py \
      --four_config_benchmark \
      --hf_parallel_workers ${HF_PARALLEL_LOADING_WORKERS}
    
    echo "=== Four-Configuration Benchmark Complete ==="
    
    # Display four-config results
    if [ -f "/tmp/four_config_benchmark_results.json" ]; then
      echo "=== FOUR-CONFIGURATION BENCHMARK RESULTS ==="
      cat /tmp/four_config_benchmark_results.json | python3 -m json.tool
    fi
  else
    echo "=== Six-Configuration Loading Benchmark (Default) ==="
    python3 train.py \
      --hf_parallel_workers ${HF_PARALLEL_LOADING_WORKERS}
    
    echo "=== Six-Configuration Benchmark Complete ==="
    
    # Display six-config results
    if [ -f "/tmp/six_config_benchmark_results.json" ]; then
      echo "=== SIX-CONFIGURATION BENCHMARK RESULTS ==="
      cat /tmp/six_config_benchmark_results.json | python3 -m json.tool
    fi
  fi
  
  echo "=== All Benchmarks Complete ==="
  echo "Results saved in:"
  if [ "$FOUR_CONFIG_ONLY" = "true" ]; then
    echo "  - Four-configuration results: /tmp/four_config_benchmark_results.json"
  else
    echo "  - Six-configuration results: /tmp/six_config_benchmark_results.json"
  fi

config:
  kubernetes:
    pod_config:
      spec:
        containers:
        - securityContext:
            capabilities:
              add:
              - IPC_LOCK
              - SYS_ADMIN
