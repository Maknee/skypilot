# Comprehensive dataset loading benchmarking with multiple formats and sources
#
# Usage:
#
#  S3_BUCKET=your-bucket sky launch data_loading.yaml -c data-loading-benchmark
#
# This config compares dataset loading performance across:
# - Storage: S3 vs NVMe vs HuggingFace Datasets
# - Formats: CSV vs Parquet vs HDF5 vs PyTorch tensors
# - Sizes: Small (1K), Medium (100K), Large (1M+ samples)
# - Loading strategies: Different workers, batch sizes, caching

envs:
  S3_BUCKET: s3://henry-test-data-bucket  # S3 bucket for datasets
  HF_TOKEN:  # Add your HuggingFace token if needed
  DATASET_CACHE_DIR: /tmp/hf_cache  # HuggingFace datasets cache
  TORCH_HOME: /tmp/torch_cache      # PyTorch model cache

resources:
  infra: k8s
  accelerators: H100:8
  network_tier: best
  disk_tier: best

num_nodes: 1

# Configure buckets for datasets with both S3 and local NVMe
file_mounts:
  /datasets_s3:
    source: ${S3_BUCKET}
    mode: MOUNT_CACHED  # Cache datasets for faster access
  # Mount the training script
  /app/train.py: ./data_loading/train.py
  /app/generate_datasets.py: ./data_loading/generate_datasets.py

setup: |
  # Install required packages
  pip install torch torchvision transformers accelerate datasets huggingface_hub
  pip install pandas pyarrow h5py pillow
  pip install nvitop psutil memory_profiler
  sudo apt install -y htop vim sysstat

  # Create directories
  mkdir -p /tmp/datasets_nvme
  mkdir -p /app
  mkdir -p ${DATASET_CACHE_DIR}
  mkdir -p ${TORCH_HOME}
  
  # Make scripts executable
  chmod +x /app/train.py
  chmod +x /app/generate_datasets.py

  echo "=== Setting up Dataset Loading Benchmark Environment ==="
  
  # Set HF token if provided
  if [ ! -z "$HF_TOKEN" ]; then
    huggingface-cli login --token $HF_TOKEN
  fi
  
  echo "=== Checking for existing datasets in S3 bucket: $S3_BUCKET ==="
  
  # Check if S3 bucket has datasets
  dataset_count=$(find /datasets_s3 -name "*.csv" -o -name "*.parquet" -o -name "*.h5" -o -name "*.pt" 2>/dev/null | wc -l)
  echo "Found $dataset_count existing dataset files in S3"
  
  if [ "$dataset_count" -eq 0 ]; then
    echo "No existing datasets found. Generating benchmark datasets..."
    
    # Generate datasets in multiple formats
    python3 /app/generate_datasets.py \
      --s3_dir /datasets_s3 \
      --nvme_dir /tmp/datasets_nvme \
      --generate_all
    
  else
    echo "✅ Found existing datasets in S3, skipping generation"
    ls -lh /datasets_s3/
  fi
  
  # Download some popular HuggingFace datasets for comparison
  echo "=== Downloading HuggingFace datasets for comparison ==="
  python3 -c "
from datasets import load_dataset
import os
os.environ['HF_DATASETS_CACHE'] = '$DATASET_CACHE_DIR'

# Download small datasets for quick benchmarking
try:
    print('Downloading imdb dataset...')
    load_dataset('imdb', split='train[:1000]', cache_dir='$DATASET_CACHE_DIR')
    print('Downloading cifar10 dataset...')
    load_dataset('cifar10', split='train[:1000]', cache_dir='$DATASET_CACHE_DIR')
    print('✅ HuggingFace datasets downloaded successfully')
except Exception as e:
    print(f'⚠️ Some HF datasets failed to download: {e}')
"
  
  echo "✅ Setup complete. Available datasets:"
  echo "S3 datasets:"
  find /datasets_s3 -name "*.csv" -o -name "*.parquet" -o -name "*.h5" -o -name "*.pt" | head -10
  echo "NVMe datasets:"
  find /tmp/datasets_nvme -name "*.csv" -o -name "*.parquet" -o -name "*.h5" -o -name "*.pt" | head -10

run: |
  cd /app
  
  echo "=== Starting Comprehensive Dataset Loading Benchmark ==="
  echo "Comparing multiple data loading strategies:"
  echo "  1. Storage: S3 vs NVMe vs HuggingFace Datasets"
  echo "  2. Formats: CSV vs Parquet vs HDF5 vs PyTorch tensors"
  echo "  3. Sizes: Small (1K) vs Medium (100K) vs Large (1M+ samples)"
  echo "  4. Workers: 1, 4, 8, 16 data loading workers"
  echo "  5. Batch sizes: 16, 32, 64, 128"
  
  # List available datasets
  echo "Available datasets:"
  echo "S3 datasets:"
  find /datasets_s3 -type f | head -20
  echo "NVMe datasets:"
  find /tmp/datasets_nvme -type f | head -20
  
  # Run comprehensive dataset loading benchmark
  accelerate launch \
    --num_processes 8 \
    --num_machines 1 \
    --machine_rank 0 \
    --main_process_ip 127.0.0.1 \
    --main_process_port 29500 \
    train.py \
    --s3_data_dir /datasets_s3 \
    --nvme_data_dir /tmp/datasets_nvme \
    --hf_cache_dir ${DATASET_CACHE_DIR} \
    --epochs 2 \
    --benchmark_iterations 5 \
    --test_all_formats \
    --test_all_sizes \
    --test_worker_configs "1,4,8,16" \
    --test_batch_sizes "16,32,64,128"
  
  echo "=== Dataset Loading Benchmark Complete ==="
  
  # Display final results
  if [ -f "/tmp/data_loading_benchmark_results.json" ]; then
    echo "=== FINAL COMPREHENSIVE BENCHMARK RESULTS ==="
    cat /tmp/data_loading_benchmark_results.json | python3 -m json.tool
  fi
  
  # Generate performance report
  if [ -f "/tmp/data_loading_benchmark_results.json" ]; then
    echo "=== Generating Performance Analysis Report ==="
    python3 train.py --analyze_results_only
  fi
  
  # Run memory usage analysis
  echo "=== Running Memory Usage Analysis ==="
  python3 train.py \
    --s3_data_dir /datasets_s3 \
    --nvme_data_dir /tmp/datasets_nvme \
    --hf_cache_dir ${DATASET_CACHE_DIR} \
    --memory_benchmark_only \
    --test_batch_sizes "32,64,128,256"

config:
  kubernetes:
    pod_config:
      spec:
        containers:
        - securityContext:
            capabilities:
              add:
              - IPC_LOCK
              - SYS_ADMIN
          resources:
            requests:
              memory: "32Gi"
            limits:
              memory: "64Gi"
